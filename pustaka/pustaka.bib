Untuk setiap referensi, baris pertama (Newton1687) merupakan label dan baris-baris berikutnya
merupakan atribut dari referensi yang akan digunakan. Umumnya, format referensi berikut sudah
disediakan oleh penyedia sumber referensi secara online dan bisa langsung digunakan di sini.

Lihat https://www.bibtex.com/g/bibtex-format/ untuk informasi lebih lanjut mengenai format
BibTex yang digunakan pada file ini.

@inproceedings{6830928,
  author = {AlJahdali, Hussain and Albatli, Abdulaziz and Garraghan, Peter and
            Townend, Paul and Lau, Lydia and Xu, Jie},
  booktitle = {2014 IEEE 8th International Symposium on Service Oriented System
               Engineering},
  title = {Multi-tenancy in Cloud Computing},
  year = {2014},
  volume = {},
  number = {},
  pages = {344-351},
  keywords = {Security;Cloud computing;Resource management;Computational
              modeling;Virtualization;Databases;Servers;Cloud
              Computing;Security;Multi-Tenancy;Attack Models;Cloud Data},
  doi = {10.1109/SOSE.2014.50},
}

@inproceedings{43438,
  title = {Large-scale cluster management at {Google} with {Borg}},
  author = {Abhishek Verma and Luis Pedrosa and Madhukar R. Korupolu and David
            Oppenheimer and Eric Tune and John Wilkes},
  year = {2015},
  booktitle = {Proceedings of the European Conference on Computer Systems
               (EuroSys)},
  address = {Bordeaux, France},
}

@misc{kubernetes-website-multi-tenancy,
  author = {{Kubernetes}},
  title = {Multi-tenancy},
  year = {2024},
  url = {https://kubernetes.io/docs/concepts/security/multi-tenancy},
  note = {Accessed: 2024-12-01},
}

@misc{kubernetes-website-components,
  author = {{Kubernetes}},
  title = {Components},
  year = {2024},
  url = {https://kubernetes.io/docs/concepts/overview/components/},
  note = {Accessed: 2024-12-02},
}

@misc{kubernetes-website-hpa,
  author = {{Kubernetes}},
  title = {Horizontal-Pod-Autoscale},
  year = {2024},
  url = {https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/},
  note = {Accessed: 2024-12-10},
}

@misc{kubernetes-website-adding-linux-node,
  author = {{Kubernetes}},
  title = {adding-linux-node},
  year = {2024},
  url = {https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/adding-linux-nodes/},
  note = {Accessed: 2024-12-10},
}

@phdthesis{oliva_multi-tenancy_2024,
	type = {laurea},
	title = {Multi-{Tenancy} in {Kubernetes} {Clusters}},
	copyright = {cc\_by\_nc\_nd},
	url = {https://webthesis.biblio.polito.it/33340/},
	abstract = {The growing demand for Kubernetes clusters and the increasing computational investments for 5G telecommunication infrastructure are leading to a big opportunity: competitiveness in the cloud computing market sector. Telcos could enter this market by leveraging spare resources from their infrastructure and providing Kubernetes clusters as a service. Nevertheless, a novel obstacle is presented by the Kubernetes design, as it was not designed with multi-tenancy in mind. In order to overcome this limitation, the conventional provisioning approach involves the installation of a dedicated cluster within Virtual Machines for each tenant.    Conventional provisioning techniques frequently result in resource waste and higher operating expenses to maintain tenant isolation. With an emphasis on resource allocation optimization and scalability enhancement while maintaining tenant isolation, this thesis explores resource-efficient multi-tenancy strategies for Kubernetes clusters, especially in the context of bare-metal deployments. The investigation resorted to adding multi-tenancy capabilities to a Kubernetes cluster, since there was no viable technology to enhance dedicated clusters. To accomplish this, a collection of technologies is needed to implement Kubernetes control and data plane isolation. One of the most intriguing control plane isolation techniques to emerge is the concept of virtual clusters. This approach enables the sharing of a single Kubernetes cluster by deploying specialized components that, while appearing as independent entities, primarily delegate operations to the underlying shared cluster. Meanwhile, the only data plane isolation that has been researched is pod sandboxing, which uses containers inside virtual machines (VMs) and is the most practical method in this situation. After comparing dedicated and shared cluster solutions, it was proved that the virtual cluster with pod sandboxing required fewer resources and produced a workload that was more efficient.    Another significant challenge in practice is the management of multiple tenant clusters. While multiple tenant clusters can be deployed on a single bare-metal cluster, the complexity increases when managing multiple clusters across different bare-metal environments. This work provides a concise overview of multi-cluster management strategies based on ClusterAPI, progressing from basic methods to more scalable and resilient solutions using Hosted Control Planes.},
	language = {it},
	urldate = {2024-11-29},
	school = {Politecnico di Torino},
	author = {Oliva, Attilio},
	month = oct,
	year = {2024},
}

@inproceedings{6133210,
  author={Doelitzscher, Frank and Held, Markus and Reich, Christoph and Sulistio, Anthony},
  booktitle={2011 IEEE Third International Conference on Cloud Computing Technology and Science}, 
  title={ViteraaS: Virtual Cluster as a Service}, 
  year={2011},
  volume={},
  number={},
  pages={652-657},
  keywords={Benchmark testing;Cloud computing;Monitoring;Quality of service;Kernel;Servers;Computational modeling;cloud computing;virtualization;virtual cluster;HPC},
  doi={10.1109/CloudCom.2011.101}
}

@inproceedings{9546524,
  author={Zheng, Chao and Zhuang, Qinghui and Guo, Fei},
  booktitle={2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)}, 
  title={A Multi-Tenant Framework for Cloud Container Services}, 
  year={2021},
  volume={},
  number={},
  pages={359-369},
  keywords={Cloud computing;Conferences;Software as a service;Computer architecture;Containers;Throughput;IP networks;Distributed architecture},
  doi={10.1109/ICDCS51616.2021.00042}
}
